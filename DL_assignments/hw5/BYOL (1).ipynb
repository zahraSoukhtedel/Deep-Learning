{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnKVArh_0qhm"
      },
      "source": [
        "# BYOL\n",
        "\n",
        "In this notebook we are going to implement [BYOL: Bootstrap Your Own Latent](https://arxiv.org/pdf/2006.07733.pdf) and compare the results of a classification task before and after pretraining the model with BYOL."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive//My Drive/Colab Notebooks/Deep_hw1/resnet.py')\n",
        "import resnet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmOV36bbJoCu",
        "outputId": "6d2ba98d-6266-4b1a-9a94-31cbf19bd2a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E6nh_5Q2uIp"
      },
      "source": [
        "### Data Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B7vzr4sTnriT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import Callable, Tuple\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, Tensor\n",
        "from torchvision import transforms as T\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn: Callable, p: float):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return x if torch.rand(1).item() > self.p else self.fn(x)\n",
        "\n",
        "\n",
        "def default_augmentation(image_size: Tuple[int, int] = (224, 224)) -> nn.Module:\n",
        "    \"\"\"\n",
        "        1. resize images to 'image_size'\n",
        "        2. RandomApply color jitter\n",
        "        3. RandomApply grayscale\n",
        "        4. RandomApply horizon flip\n",
        "        5. RandomApply gaussian blur with kernel_size(3, 3), sigma=(1.5, 1.5)\n",
        "        6. RandomApply ResizedCrop to 'image_size'\n",
        "        7. Normalize\n",
        "        choosing hyperparameters that are not mentioned is up to you\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        T.Resize(image_size),\n",
        "        RandomApply(\n",
        "            T.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
        "            p=0.8\n",
        "        ),\n",
        "        RandomApply(\n",
        "            T.Grayscale(num_output_channels=3),\n",
        "            p=0.2\n",
        "        ),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        RandomApply(\n",
        "            T.GaussianBlur(kernel_size=3, sigma=(1.5, 1.5)),\n",
        "            p=0.1\n",
        "        ),\n",
        "        T.RandomResizedCrop(image_size),\n",
        "        T.Normalize(\n",
        "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
        "            std=torch.tensor([0.229, 0.224, 0.225]),\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8msgADLGI1P"
      },
      "source": [
        "# Model\n",
        "We will use ResNet18 as our representation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YoU-3B4FmtrZ"
      },
      "outputs": [],
      "source": [
        "def get_encoder_model():\n",
        "    resnet = torchvision.models.resnet18(pretrained=False)\n",
        "    # remove last fully-connected layer\n",
        "    #resnet.fc = nn.Identity()\n",
        "    resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "    return resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBSjJ4ZYGI1P"
      },
      "source": [
        "### Loss Function\n",
        "We need to use NormalizedMSELoss as our loss function.\n",
        "$$NormalizedMSELoss(v_1, v_2) = \\Vert \\bar{v_1} - \\bar{v_2}\\Vert_2^2 = 2 - 2.\\frac{\\langle v_1, v_2 \\rangle}{\\Vert v_1\\Vert_2 \\Vert v_2\\Vert_2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7fFhLSZjGI1Q"
      },
      "outputs": [],
      "source": [
        "class NormalizedMSELoss(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(NormalizedMSELoss,self).__init__()\n",
        "\n",
        "    def forward(self, view1: Tensor, view2: Tensor) -> Tensor:\n",
        "        view1_norm = F.normalize(view1, p=2, dim=-1)\n",
        "        view2_norm = F.normalize(view2, p=2, dim=-1)\n",
        "        return 2 - 2 * (view1_norm * view2_norm).sum(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8V06lymGI1Q"
      },
      "source": [
        "### MLP\n",
        "Here you will implement a simple MLP class with one hidden layer with BatchNorm and ReLU activation, and a linear output layer. This class will be used for both the projections and the prediction networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cO5luEjwGI1Q"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, projection_dim: int = 256, hidden_dim: int = 4096) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRzbK0ODGI1Q"
      },
      "source": [
        "### Encoder + Projector Network\n",
        "This is the network structure that is shared between online and target networks. It consists of our encoder model, followed by a projection MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "m5P_YTPYGI1R"
      },
      "outputs": [],
      "source": [
        "class EncoderProjecter(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 hidden_dim: int = 4096,\n",
        "                 projection_out_dim: int = 256\n",
        "                 ) -> None:\n",
        "        super(EncoderProjecter, self).__init__()\n",
        "\n",
        "        # your code\n",
        "        self.encoder = encoder\n",
        "        self.projector = MLP(encoder.output_dim, projection_out_dim, hidden_dim)\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # your code\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.projector(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjSCG2qCGI1R"
      },
      "source": [
        "## BYOL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vBTUWyB9GI1R"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class BYOL(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 hidden_dim: int = 4096,\n",
        "                 projection_out_dim: int = 256,\n",
        "                 target_decay: float = 0.99\n",
        "\n",
        "                ) -> None:\n",
        "        super(BYOL, self).__init__()\n",
        "\n",
        "        # your code\n",
        "        self.online_network = EncoderProjecter(model, hidden_dim, projection_out_dim)\n",
        "        self.online_predictor = MLP(projection_out_dim, projection_out_dim, hidden_dim)\n",
        "\n",
        "        self.target_network = copy.deepcopy(self.online_network)  # init with copy of parameters of online network\n",
        "        # set target_network's weights to be untrainable\n",
        "\n",
        "        self.target_network.eval()\n",
        "\n",
        "        for param in self.target_network.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.loss_function = NormalizedMSELoss()\n",
        "        self.target_decay = target_decay\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def soft_update_target_network(self) -> None:\n",
        "        for online_params, target_params in zip(self.online_network.parameters(), self.target_network.parameters()):\n",
        "            target_params.data = self.target_decay * target_params.data + (1 - self.target_decay) * online_params.data\n",
        "\n",
        "\n",
        "    def forward(self, view) -> Tuple[Tensor]:\n",
        "        online_projection = self.online_network(view)\n",
        "        target_projection = self.target_network(view)\n",
        "        return online_projection, target_projection\n",
        "\n",
        "\n",
        "    def loss(self, view1, view2):\n",
        "        online_projection1, target_projection1 = self.forward(view1)\n",
        "        online_projection2, target_projection2 = self.forward(view2)\n",
        "\n",
        "        online_prediction1 = self.online_predictor(online_projection1)\n",
        "        online_prediction2 = self.online_predictor(online_projection2)\n",
        "\n",
        "        loss1 = self.loss_function(online_prediction1, target_projection2.detach())\n",
        "        loss2 = self.loss_function(online_prediction2, target_projection1.detach())\n",
        "\n",
        "        return (loss1 + loss2) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQzcP89G4L2u"
      },
      "source": [
        "# STL10 Datasets\n",
        "\n",
        "We need 3 separate datasets from STL10 for this experiment:\n",
        "1. `\"train\"` -- Contains only labeled training images. Used for supervised training.\n",
        "2. `\"train+unlabeled\"` -- Contains training images, plus a large number of unlabelled images.  Used for self-supervised learning with BYOL.\n",
        "3. `\"test\"` -- Labeled test images.  We use it both as a validation set, and for computing the final model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lTVx52t9Kjf",
        "outputId": "a9e7cf71-8d31-42c9-df44-38bcc57577ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import STL10\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "TRAIN_DATASET = STL10(root=\"data\", split=\"train\", download=True, transform=ToTensor())\n",
        "TRAIN_UNLABELED_DATASET = STL10(root=\"data\", split=\"train+unlabeled\", download=True, transform=ToTensor())\n",
        "TEST_DATASET = STL10(root=\"data\", split=\"test\", download=True, transform=ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0VilOCNGI1S"
      },
      "source": [
        "Create dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OEaChXnQGI1S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(TRAIN_DATASET, batch_size=64, shuffle=True)\n",
        "train_unlabeled_dataloader = DataLoader(TRAIN_UNLABELED_DATASET, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(TEST_DATASET, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdlrDnwR45bm"
      },
      "source": [
        "# Supervised Training without BYOL\n",
        "\n",
        "First create a classifier model by simply adding a linear layer on top of the encoder model. Then train the model using the labeled training set. Performance should be pretty good already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijpDGufWGI1S",
        "outputId": "9332140e-92b2-413f-a36b-79e8df2225f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "encoder = get_encoder_model()\n",
        "classifier = nn.Sequential(\n",
        "    encoder,\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(512, 10)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o5563ZJ1GI1T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBTyX-f45Sgj"
      },
      "source": [
        "### Self-Supervised Training with BYOL\n",
        "\n",
        "Now perform the self-supervised training. This is the most computationally intensive part of the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "zyJ4KJymGI1T",
        "outputId": "74881bcf-9081-49f3-f00e-45da2f9ec95d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0c3847de00a4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize BYOL and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbyol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBYOL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f6beec97d3b4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, hidden_dim, projection_out_dim, target_decay)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderProjecter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_out_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_out_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_out_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-9723077260b2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, encoder, hidden_dim, projection_out_dim)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_out_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'output_dim'"
          ]
        }
      ],
      "source": [
        "# Initialize BYOL and optimizer\n",
        "byol = BYOL(encoder)\n",
        "optimizer = torch.optim.Adam(byol.parameters(), lr=3e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_unlabeled_dataloader:\n",
        "        view1, view2 = data['image1'], data['image2']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = byol.loss(view1, view2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        byol.soft_update_target_network()\n",
        "\n",
        "    print(\"Epoch: {}/{}, Loss: {:.4f}\".format(epoch + 1, num_epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjMmW9Tj5k-j"
      },
      "source": [
        "### Supervised Training Again\n",
        "\n",
        "Extract the encoder network's state dictionary from BYOL, and load it into our ResNet18 model before starting training.  Then run supervised training, and watch the accuracy improve from last time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT3ueQrcBeTZ"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained weights from BYOL\n",
        "encoder.load_state_dict(byol.online_network.encoder.state_dict())\n",
        "\n",
        "# Train the classifier using the labeled data\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classifier(images)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the classifier on the test set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            outputs = classifier(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(\"Epoch: {}/{}, Loss: {:.4f}, Test Accuracy: {:.2f}%\".format(epoch + 1, num_epochs, loss.item(), 100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7L7lUxcMGwoa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}